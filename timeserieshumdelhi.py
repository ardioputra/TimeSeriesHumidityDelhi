# -*- coding: utf-8 -*-
"""TimeSeriesHumDelhi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TPbpF2jZa3D8tBPFfXoMMsKp3QDf0k7K

#**Submission 2**

##Ardio Pratama Putra
"""

import pandas as pd
dt = pd.read_csv('testset.csv')
dt.head()

dt.isnull().sum()

dt['datetime_utc'] = pd.to_datetime(dt['datetime_utc'])
dt[' _hum'].fillna(dt[' _hum'].mean(), inplace=True)
dt = dt[['datetime_utc',' _hum']]
dt.head()

dt['datetime_utc'] = dt['datetime_utc'].dt.date
dt = dt.rename(columns={'datetime_utc':'date',' _hum':'hum'})
dt.head()

dt.shape

date = dt['date'].values
hum = dt['hum'].values

import matplotlib.pyplot as plt

plt.figure(figsize=(25,10))
plt.plot(date, hum)
plt.title('Data Kelembapan di Delhi 1996-2017')
plt.xlabel('Date')
plt.ylabel('Hum')
plt.show()

import tensorflow as tf

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series,axis=1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(hum, date, test_size = 0.2, shuffle = False, random_state=0)

from keras.layers import Dense, LSTM

train_set = windowed_dataset(x_train, 60, 100, 5000)
test_set = windowed_dataset(x_test, 60, 100, 5000)

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Conv1D(strides=1,
                                                           kernel_size=5, 
                                                           filters=32, 
                                                           padding="causal",
                                                           input_shape=[None, 1],
                                                           activation="relu"),
                                    tf.keras.layers.LSTM(60, return_sequences=True),
                                    tf.keras.layers.LSTM(60, return_sequences=True),
                                    tf.keras.layers.Dense(30, activation='relu'),
                                    tf.keras.layers.Dense(10, activation='relu'),
                                    tf.keras.layers.Dense(1),
                                    tf.keras.layers.Lambda(lambda x: x * 400)
])
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])

max = dt['hum'].max()
min = dt['hum'].min()
data_scale = (max - min)*0.1
print('MAE data scale below 10% is', data_scale)

class MAECallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<data_scale):
      self.model.stop_training = True
      print("\nBelow 10% Data Scale")
callbacks = MAECallback()

hist = model.fit(train_set, validation_data=test_set, epochs=100, callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

plt.plot(hist.history['mae'])
plt.plot(hist.history['val_mae'])
plt.title('MAE Model')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()